# ERNIE: Enhanced Language Representation with Informative Entities
> May 2019
> Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu (ACL 2019)
> Affiliation: Tsinghua University, Huawei Noah's Ark
> Paper: [Link](https://arxiv.org/abs/1905.07129)
> Code: [GitHub](https://github.com/thunlp/ERNIE)

[toc]

## Introduction & Related Works

Pre-trained language models (e.g. word2vec, GloVe, GPT, BERT) have been acheiving promising results in recent years, they neglect  to  incorporate  knowledge  information for language understanding.

![](https://i.imgur.com/AMB65UY.png)

Take for example the sentence "*Bob Dylan* wrote *Blowin' in the Wind* in 1962, and wrote *Chronicles: Volume One* in 2004". It is difficult for a model to know that **Bob Dylan** is a the **composer** and **author** if it doesn't know it is a *Song* and *Book*. Which makes it even harder to learn that **Bob Dylan**'s occupations are **Songwriter** and **Writer**.

Recent studies also show that Knowledge Integration is helpful for downstream NLP tasks such as Machine Translation, QA, Natural Language Inference and Neural Conversational Models.

In order to integrate knowledge graphs into word representations, there are **two** challenges: 1) Structured Knowledge Encodings: How to extract and encode related informative facts. 2) Heterogeneous Information Fusion: How to fuse the knowledge encodings into the word representations.

1) Structured Knowledge Encodings are generated by extracting named entities in text (TAGME), and align with corresponding entities that are encoded via TransE.
2) Heterogeneous Information Fusion: In order to integrate knowledge encodings into ERNIE, a new pre-training objective (dEA) was proposed. Which objective requires the model learn the correct entities from context and knowledge graph.

## Methodology

### Model Architecture

![](https://i.imgur.com/McUS2Zf.png)

ERNIE consists of two stacked models, a textual encoder, *T-Encoder* and a knowledge encoder, *K-Encoder*. Where the *T-Encoder* is identical to BERT. Both *T-Encoder* and *K-Encoder* have 6 layers, denoted as $N$ & $M$ respectively

After encoding basic lexical and syntactic information from *T-Encoder*, the tokens are the used as the token input, along with pre-trained embeddings of TransE as entity input for the *K-Encoder*.

The token input and entity input first performs self-attention (two individual multihead-attention), linear-transformation and concat the entity embeddings to the first token of the entity tokens $f(w_i) = e_j$. And perform the below operation

$$
\begin{align*}
\mathbf{h}_i &= \sigma(\mathbf{W}_{1,1}^{(l)} \tilde{\mathbf{w}}_i^{(l)} + \mathbf{W}_{1,2}^{(l)} \tilde{e}_j^{(l)} + \mathbf{b}_1^{(l)}) &\text{ intermediate} \\
\mathbf{w}_i^{(l)} &= \sigma(\mathbf{W}_2^{(l)} \mathbf{h}_i + \mathbf{b}_2^{(l)}) \qquad \mathbf{e}_j^{(l)} = \sigma(\mathbf{W}_3^{(l)} \mathbf{h}_i + \mathbf{b}_3^{(l)}) &\text{ output} \\
\end{align*}
$$

Where $\mathbf{h}$ is the integration of the entity embedding and token embedding and $\sigma$ is the GELU function. If there is no alignment found, the $e_j$ operation is omitted and $\mathbf{h}$ is denoted as:

$$
\begin{align*}
\mathbf{h}_i &= \sigma(\mathbf{W}_{1,1}^{(l)} \tilde{\mathbf{w}}_i^{(l)} + \mathbf{b}_1^{(l)}) \\
\mathbf{w}_i^{(l)} &= \sigma(\mathbf{W}_2^{(l)} \mathbf{h}_i + \mathbf{b}_2^{(l)}) \\
\end{align*}
$$

And finally outputs the token embedding $\mathbf{w}_i^{(l)}$ and entity embedding $\mathbf{e}_j^{(l)}$

### Pre-training

The pre-training objective for ERNIE is (NSP + MLM), similar to BERT, with the addition of Denoising Entity Autoencoder. The main objective of dEA is to predict appropriate entities to complete the alignments. Where...

- 5% Replace entity with random entity
  - Model learns to correct the errors that the token is aligned with a wrong entity
- 15% Mask entities
  - Model learns to correct the errors that the entity alignment system does not extract all existing alignment

<p style="page-break-before: always">

- 80% Token entity alignments unchanged
  - Model learns to integrate the entity information into token representations

Where the prediction is based on the entities within a batch `[entity_num_in_the_batch, dim]`, and not the whole list of entities $E$ as it would be too big for the softmax layer.
The cross-entropy is calculated after a softmax function. The pre-trained task is trained on Wikipedia Corpus and Wikidata.
$$
p(e_j|w_i) = \frac{\exp(\text{linear}(\mathbf{w}_i^o) \cdot \mathbf{e}_j)}{\sum_{k=1}^m \exp(\text{linear}(\mathbf{w}_i^o) \cdot \mathbf{e}_k)}
$$

### Fine-tuning

![](https://i.imgur.com/GBaD49h.png)

### Hyperparameters Details

![](https://i.imgur.com/4Duqxyh.png)

Pre-training:

- epoch: $1$
- max_seq_len: $512 \to 256$
- batch_size: $256 \to 512$
- learning rate: $5e^{-5}$
- optimizer: Adam

Fine-tuning:

- epoch: $3 \text{ ~ } 10$
- batch_size: $32$
- learning_rate: $5e^{-5}, 3e^{-5}, 2e^{-5}$
- optimizer: Adam

FIGER:

- epoch: $2 \text{ ~ } 3$
- batch_size: $2048$


## Experiments

The model was experimented on three main tasks, Entity Typing, Relation Classification, and General NLP tasks in GLUE. The Entity Typing task is to predict the semantic type of the entity mention (e.g. Bob Dylan $\to$ Writer). The Relation Classification task is to predict the relation between two entities (e.g. Bob Dylan, Blowin' in the Wind $\to$ Composer).

### Entity Typing

There are two datasets used for evaluating this task. 
1) FIGER: Training set is labeled with distant supervision, and test set is annotated by human (113 Types, 2M/10K/563 examples). Which uses the F1-metric

![](https://i.imgur.com/2aTmUAQ.png =360x)

We could see that BERT performs very well in the Macro and Micro scores, but has lower accuracy as the predicted instances are strictly compared to human annotations. ERNIE on the other hand uses external knowledge that prevents BERT from learning noisy labels from distant supervision.

2) OE: Fully human annotated via crowd sourcing (6 Types, 2000/2000/2000 examples)
![](https://i.imgur.com/00XXngP.png =380x)

We could see that pre-trained language models have better recall scores which suggests that pre-trained language models perform when the entity types are more non-strict, where ERNIE boosts BERT by 2% in both precision and recall scores.

<p style="page-break-before: always">

### Relation Classification

There are also two datasets used for evaluating this task. 

![](https://i.imgur.com/YQD8Fc3.png =360x)

1) FewRel: Few-shot learning ((64, 16, 20) relations, 100 examples for each relation, 8K/16K/16K)

FewRel (also from THU) does not have any null instance where there is not any relation between entities. Since Few-Rel is based on Wikidata, relations that exist in FewRel & Wikidata are dropped during pre-training. Only compared to CNN, which is the baselines for FewRel, and fails to learn from small datasets.

2) TACRED: Human annotated (42 relations, 78% negative, 119,474 examples, 68K/22K/15K)

Nearly 80% of the relations are null instances. Results are compared with the best results of CNN, RNN, and GCN models respectively. The GCN model performs better in TACRED as it uses an entity masking approach similar to ERNIE.

SOTA: Matching the Blanks (Google, as of 2020 March)

### GLUE

After integrating knowledge representations into BERT, ERNIE still performs on par in terms of GLUE tasks. On the one  hand, it means GLUE does not require external knowledgefor language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.

![](https://i.imgur.com/Di0iWWS.png =360x)

### Ablation Study

The ablation study was done on FewRel which explores the effects on adding entities and dEA during pre-training. It shows that adding entity sequences increases the model's F1 score by 0.7 dEA increases the model's F1 score by 0.9

![](https://i.imgur.com/YNCIZ1G.png =360x)