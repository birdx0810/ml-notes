# A Neural Knowledge Language Model

> Sungjin Ahn, Heeyoul Choi, Tanel PÃ¤rnamaa, Yoshua Bengio (~~ICLR 2017~~/Rejected)
> Affiliation: Universite de Montreal
> Paper: [Link](https://openreview.net/forum?id=BJwFrvOeg)
> Code: ~~NONE~~

## Introduction
- Traditional models are good at capturing statistical co-occurrences of entities
    - as long as observed frequently in the corpus
    - feeding a larger corpus into a bigger model 
- Reasons for learning difficulty
    - rare and unknown words (OOV)
    - computationally prohibitive
    - facts could change over time
- A good language model should exercise some level of reasoning

Proposed ***Neural Knowledge Language Model***
- Incorporating symbolic knowledge from knowledge graphs into an RNNLM 
- NLKM predicts if a word is based on a fact during each step of generation
- This work introduces a combination of a LM with knowledge based retrieval system on topical knowledge

***Contributions***
- resolve limitations of traditional language models in dealing with factual knowledge
- develops new dataset that aligns text to facts, [WikiFact](https://bitbucket.org/skaasj/wikifact_filmactor/src/master/) 

## Methodology
![](https://i.imgur.com/L05bEez.png =480x)

- For each topic $k \in K$, we are given a topic knowledge and topic description $\{(\mathcal{F}_k, W_k)\}_{k=1}^K$
- $\mathcal{F} = \{a^1, ..., a^{|\mathcal{F}|}\}$ is a set of *topic knowledge* facts on a particular topic
    - obtained from Freebase
    - subject entity $=$ topic entity
- $W = \{w_1, ..., w_{|W|}\}$ is a sequence of words that describes the topic (*topic description*)
    - obtained from Wikipedia
- $\mathcal{O}_a = \{o_1^a, ..., o_N^a\}$ is a sequence of *knowledge words* of a fact $a$ which we could retrieve to generate an output
    - $\mathcal{O} = (o^a_1=\text{"Michele"}, \  o^a_1=\text{"Obama"})$
    - only entities (object and subject), as relationships are easy to generate
    - subject entities are accesed via a special fact $\text{(Topic, Topic_Itself, Topic)}$


### Alignment
- Supervised using simple string alignment
    - Aligning words in topic description with their corresponding facts
- Given $\mathcal{F}$ and $W$ for a topic, we associate fact $a$ to a word $w$ if it appears in knowledge words $\mathcal{O}_a$, and construct a sequence of augmented observations $Y = \{y_t=(w_t,a_t,z_t)\}_{t=1:|W|}$
- Words that are not associated to facts are mapped to a "fact-type" Not-a-Fact ($\text{NaF}$)

### Knowledge Memory
- We assume the topic knowledge $\mathcal{F}$ is loaded into the knowledge memory in the form of matrix $F \in \mathbb{R}^{D_a \times |\mathcal{F}|}$, 
- The $i$-th column is the fact embedding $a^i \in \mathbb{R}^{D_a}$, which is a fixed (during training) embedding obtained via TransE.

### Inference
1. Input the word and fact prediction from previous time-stamp and update the LSTM controller
    - $\text{x}_t = \text{concat}(a_{t-1}. w^v_{t-1}, w^o_{t-1})$
    - $w^v_{t-1}$ and $w^o_{t-1}$ is a zero vector if it was not generated in the previous step
2. Given output of LSTM, predict fact and extract corresponding embedding
     - $h_t = \text{LSTM}(h_{t-1}, \text{x}_t)$
     - $P(a|h_t) = \text{softmax}(k_\text{fact}^\intercal, \mathbf{F}[a])$
     - $a_t = argmax_{a \in \mathcal{F}}\Pr(a|h_t, e_k)$
     - $\mathbf{a}_t = \mathbf{F}[a_t]$ (knowledge memory lookup)
     - Where, $e_k$ is the **mean-pooled embedding** of a particular topic knowledge
     - $k_\text{fact}$ is generated by a $\text{ReLU}(\text{MLP}(h_t, e_k))$ function
3. Determine source of word generation from extracted fact embedding and LSTM hidden state
    - $z_t = \mathbb{I}[p(z_t|h_t, \mathbf{a}_t) > 0.5]$ (binary classification)

$$
z_t =
\begin{cases}
1, \text{if } w_t \in \mathcal{O}_{a_t} \\
0, \text{otherwise}
\end{cases}
$$

4. Generate word from global vocabulary or copy from knowledge words
$$
w_t = 
\begin{cases}
w^v_t \in \mathcal{V}, \text{if }\hat{z}_t \text{ if} < 0.5\\
w^o_t \in \mathcal{O}_{a_t}, \text{otherwise}
\end{cases}
$$
    - From Vocabulary Softmax
        - $\Pr(w^v_t = w|h_t, a_t) = \frac{\exp(k^\intercal_{\text{vocab}} \mathbf{W}[w])}{\sum_{w' \in \mathcal{V}} k^\intercal_{\text{vocab}} \mathbf{W}[w']}$
    - From Knowledge Softmax
        - $\Pr(n|h_t, a_t) = \frac{\exp(k^\intercal_{\text{pos}} \mathbf{P}[n])}{\sum_{n'} k^\intercal_{\text{pos}} \mathbf{P}[n']}$
        - $n_t = \text{argmax}_{m=0:|\mathcal{O}_{a_t}|-1} \Pr(n|h_t, a_t)$
        - $w^o_t = \mathcal{O}_{a_t}[n_t]$

### Algorithm/Pseudocode
![](https://i.imgur.com/ssOJ8og.png =480x)

**Notes**:
- The objective is to maximise the log-likelihood of the augmented observation w.r.t the model parameter $\theta$
    - $\theta^* = \arg\max_\theta \sum_k \log P_\theta (Y_k|\mathcal{F}_k)$
    - $P_\theta(y_t|y_{1:t-1}) = P_\theta(w_t|a_t, z_t,h_t)P_\theta(a_t|h_t)P_\theta(z_t|h_t)$

## Evaluation

### Dataset
Proposed [WikiFact](https://bitbucket.org/skaasj/wikifact_filmactor/src/master/), where Wikipedia descriptions are algned with corresponding Freebase facts.

### Ablation
![](https://i.imgur.com/amOQyza.png)


### Vocab Size Comparison
![](https://i.imgur.com/Egw55wK.png)


### Unknown Penalized Perplexity
If all words in a sentence are unknown words, the perplexity score would be very low.

$$
P_{\text{UPP}}(w_unk) = \frac{P(w_{\text{unk}})}{|\mathcal{V}_{\text{total}} \ / \ \mathcal{V}_{\text{vocab}}|}
$$


## Conclusion
- NKLM outperforms the RNNLM in all perplexity measures
- Copy mechanism improves model significantly
- NKLM generates named entities leading to lesser OOV
- NKLM performs similarly to RNNLM without knowledge
- TransE is an efficient way of representing factual knowledge
- Standard perplexity is affected by the prediction accuracy on unknown words

### From [OpenReview](https://openreview.net/forum?id=BJwFrvOeg)
Pros:
 - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments. 
 
Mixed:
 - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks. 
 
Cons:
 - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.
 - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions

