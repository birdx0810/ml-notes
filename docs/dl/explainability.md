# Model Analysis

## Model Visualization

### NLP

A Multiscale Visualization of Attention in the Transformer Model. Jesse Vig. ACL 2019. [paper](https://www.aclweb.org/anthology/P19-3007/)

## Interpretation & Explainability

What Neural Networks Memorize and Why:Discovering the Long Tail via Influence Estimation. Vitaly Feldman, Chiyuan Zhang. [paper](https://arxiv.org/abs/2008.03703)

The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, Dawn Song. USENIX 2019. [paper](https://arxiv.org/abs/2008.03703)

### Image

Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman. ICLR 2014 Workshop. [paper](https://arxiv.org/abs/1312.6034)

This Looks Like That: Deep Learning for Interpretable Image Recognition. Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin. NeurIPS 2019_._ [paper](https://arxiv.org/abs/1806.10574)

ProtoPNet: This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition. Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert. 2020. [paper](https://arxiv.org/abs/2011.02863)

NP-ProtoPNet: These do not Look Like Those. Gurmail Singh and Yow Kin-Choong. 2021. [paper](https://ieeexplore.ieee.org/document/9373404)

### NLP

Attention is not Explanation. Sarthak Jain, Byron C. Wallace. NAACL 2019. [paper](https://www.aclweb.org/anthology/N19-1357/)

Attention is not not Explanation. Sarah Wiegreffe, Yuval Pinter. IJCNLP. [paper](https://www.aclweb.org/anthology/D19-1002/)

Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh. ACL 2020. [paper](https://www.aclweb.org/anthology/2020.acl-main.442/)

